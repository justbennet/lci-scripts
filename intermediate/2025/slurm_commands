# On lci-head-XX-1:
# Export your cluster number as XX so it can be used as ${XX} later
export XX=01

# You should have one shared file system.  Uncomment the one that you have
# export SHARED_FS=/mnt/beegfs/scratch
# export SHARED_FS=/mnt/lustre/scratch
export SHARED_FS=/gpfs/lci/scratch

# Using Slurm 24.11.1 as installed on the VMs for 2025 Intermediate

# To build Slurm and install Slum with the configuration below, the
# powertools repo needs to be enabled: 

dnf  config-manager --enable  powertools

# Install the build prerequisites:

dnf install -y \
  hdf5-devel \
  hwloc-devel \
  lua-devel \
  mariadb-devel \
  munge-devel \
  munge-libs \
  numactl-devel \
  pam-devel \
  pkgconf \
  pmix-devel \
  readline-devel \
  ucx-devel

wget https://download.schedmd.com/slurm/slurm-24.11.1.tar.bz2

rpmbuild -ta slurm-23.11.6.tar.bz2 \
  --with hdf5 \
  --with hwloc \
  --with lua \
  --with numa \
  --with pmix \
  --with ucx 

# The RPMs will be in located in ~/rpmbuild/RPMS/x86_64. Copy that RPMS
# directory to your scratch directory where all the nodes can access
# them

cp -a ~/rpmbuild/RPMS ${SHARED_FS}

# Munge is used by Slurm for encrypted communication among the nodes.
# Before installing and configuring Slurm, configure and start MUNGE.
# On the head node, create the munge key and then copy to the compute 
# nodes 

# Install the munge RPM on the head and compute nodes:

dnf install -y munge
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node  dnf install -y munge
done

# Create the munge key only on the head node
create-munge-key

# Copy the munge key to the compute nodes 
rsync -av /etc/munge lci-compute-${XX}-1:/etc
rsync -av /etc/munge lci-compute-${XX}-2:/etc

# Check munge permissions and ownership on compute nodes. Should be
# -r-------- 1 munge munge 1024 Feb  1 22:23 munge.key
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node ls -l /etc/munge
done

# Enable and start munge on the head and compute nodes:

systemctl enable munge ; systemctl start munge
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node "systemctl enable munge ; systemctl start munge"
done


# Install the RPMS needed for slurmdbd and slurmctld on the head node

cd $SHARED_FS/RPMS/x86_64

# NOTE:  you must install mailx so there is a /usr/bin/mail for Slurm to use
#        or slurmctld will not start

dnf install -y \
  mailx \
  ./slurm-24.11.1-1.el8.x86_64.rpm \
  ./slurm-contribs-24.11.1-1.el8.x86_64.rpm \
  ./slurm-devel-24.11.1-1.el8.x86_64.rpm \
  ./slurm-example-configs-24.11.1-1.el8.x86_64.rpm \
  ./slurm-perlapi-24.11.1-1.el8.x86_64.rpm \
  ./slurm-slurmctld-24.11.1-1.el8.x86_64.rpm \
  ./slurm-slurmdbd-24.11.1-1.el8.x86_64.rpm

# Install the RPMs needed on the compute nodes

## Create a file with commands that can be run on the compute nodes
##   from the shared filesystem.
cat << __EOF__ > $SHARED_FS/install_slurm.sh
cd $SHARED_FS/RPMS/x86_64
dnf install -y \\
  ./slurm-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-contribs-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-devel-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-libpmi-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-pam_slurm-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-perlapi-24.11.1-1.el8.x86_64.rpm \\
  ./slurm-slurmd-24.11.1-1.el8.x86_64.rpm
__EOF__

## Run the script on your compute nodes
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node "bash $SHARED_FS/install_slurm.sh"
done

# On the head and compute nodes, create a Slurm system account with a
# consistent uid:gid, uid == gid, uid == 890, which was chosen from a large
# empty zone in UIDs.  Double check that it is available on yours with `id 890`

# On head node: 
adduser --system -m -u 890 -s /sbin/nologin -d /var/lib/slurm \
    -c "Slurm system account"  slurm

# Now on compute nodes:
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node adduser --system -m -s /sbin/nologin -d /var/lib/slurm \
        -c "Slurm system account" -u 990 -g slurm slurm
done

# On head node only, install the packages needed for MariaDB:
dnf instally -y mariadb mariadb-server

# Enable and start mariadb:

systemctl enable mariadb
systemctl start mariadb

# Secure the database. Run the mysql_secure_installation command which will
# ask questions to which the answers should be
#   - set the root password for the DB to be LCI2025Slurm
#       to match the password on the line
#           StoragePass=LCI2025Slurm
#       in /etc/slurm/slurmdbd.conf, which you'll set below
#   - remove anonymous users
#   - disallow root logins remotely
#   - remove test databases

mysql_secure_installation

# Connect to database, permissions for the slurm DB user, and create DB: 

# Start the interactive mysql 'shell'
mysql -p

# Enter the mariadb commands to set up the slurm database user and database
# This password must match that above
MariaDB [(none)]> create user 'slurm'@'localhost' identified by 'LCI2025Slurm';
MariaDB [(none)]> create user 'slurm'@'lci-head-XX-1' identified by 'LCI2025Slurm';
MariaDB [(none)]> grant all on slurm_acct_db.* to 'slurm'@'localhost';

# Note:  You must manually change XX to your cluster number here
MariaDB [(none)]> grant all on slurm_acct_db.* to 'slurm'@'lci-head-XX-1';
MariaDB [(none)]> create database slurm_acct_db;
MariaDB [(none)]> exit;


# On head node, Create slurmdbd.conf file. Copy slurmdbd.conf.example 
# to slurmdbd.conf

cd /etc/slurm
cp slurmdbd.conf.example slurmdbd.conf

# ... and make sure there is a line for
# StoragePass=LCI2025Slurm
# in /etc/slurm/slurmdb.conf

# Change ownership/permissions on slurmdbd.conf:
## The slurm user must own it and be the only account that can read it
chown slurm:slurm slurmdbd.conf
chmod 0600 slurmdbd.conf 

# Create log directory for slurm

mkdir -m 0750 /var/log/slurm
chown slurm:slurm /var/log/slurm

# Enable and start slurmdbd

systemctl enable slurmdbd
systemctl start slurmdbd

# Now copy slurm.conf.example to slurm.conf make these changes:

ClusterName=lci-${XX}
SlurmctldHost=lci-head-${XX}-1
SlurmctldParameters=enable_configless
DisableRootJobs=YES
EnforcePartLimits=ALL
MaxJobCount=1000
MaxStepCount=2000
RebootProgram=/usr/sbin/reboot
TaskPlugin=task/cgroup,task/affinity
SelectTypeParameters=CR_Core_Memory
PriorityWeightAge=1000
PriorityWeightFairshare=1000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=1000
AccountingStorageEnforce=limits
AccountingStorageHost=lci-head-${XX}-1
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageUser=slurm
AccountingStoreFlags=job_script
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
NodeName=lci-compute-${XX}-[1-2] CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=7685 State=UNKNOWN
PartitionName=lci Nodes=ALL Default=YES MaxTime=24:00:00 State=UP

# Create cgroup.conf from cgroup.conf.example (no changes needed)

cp cgroup.conf.example cgroup.conf

# Set correct ownership/permissions on config files

chown slurm:slurm *
chmod 0640 *
chmod 0644 slurm.conf 

# Create directory /var/spool/slurmctld

mkdir -m 0750 /var/spool/slurmctld
chown slurm:slurm /var/spool/slurmctld

# Enable and start slurmctld on head node

systemctl enable slurmctld
systemctl start slurmctld

# On the compute nodes: 
echo "SLURMD_OPTIONS=\"--conf-server lci-head-${XX}-1\"" > $SHARED_FS/etc-sysconfig-slurmd
for node in lci-compute-${XX}-1 lci-compute-${XX}-2 ; do
    ssh $node 'mkdir -m 0750 /var/log/slurm ; chown slurm:slurm /var/log/slurm'
    # Create /etc/sysconfig/slurmd with the following contents: 
    ssh $node cp $SHARED_FS/etc-sysconfig-slurmd /etc/sysconfig/slurmd
    # Enable and start slurmd:
    ssh $node 'systemctl enable slurmd ; systemctl start slurmd'
done

# Now run sinfo to check basic functionality (on any node)

# sinfo 
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
all*         up 1-00:00:00      2   idle lci-compute-XX-[1-2]

# Create accounts "lci" and "science" and add user rocky to them

sacctmgr create account lci Description="LCI Intermediate Workshop" Organization="Linux Clusters Institute" Cluster=lci-${XX}
sacctmgr create -i account science Description="Science People" Organization="LCI"
sacctmgr create user rocky cluster=lci-${XX} DefaultAccount=lci
sacctmgr add -i user rocky account=science

# Check your work with the following commands: 

sacctmgr show accounts
sacctmgr show associations account=lci format=account,user,qos

sacctmgr show users
sacctmgr show user rocky
# Show all fields for a user
sacctmgr show associations user=rocky
# You can select only some fields to show
sacctmgr show associations user=rocky format=account,user

# Create a directory for rocky on one of your shared filesystem, and 
# make it owned/writable for rocky: 

mkdir -m 0700 /mnt/beegfs/home/rocky
chown rocky:rocky $SHARED_FS/home/rocky

# Now as rocky
cd $SHARED_FS/home/rocky

# create a sbatch script and submit it: 

cat << __EOF__ > hostnames.sbatch 
#!/bin/bash

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4 
#SBATCH --job-name=hostnames
#SBATCH --time=00:01:00
#SBATCH --mem=1M
#SBATCH --output %x-%j.out

srun hostname

__EOF__

sbatch hostnames.sbatch
sleep 1
cat hostnames-*.out

cat << __EOF__ > hello.sh
#!/bin/bash
echo "Slurm asked me to say Hello from \$(hostname -s)"
__EOF__

chmod +x hello.sh
srun --nodes=2 --ntasks-per-node=2  --time=00:01:00 --mem=1M ./hello.sh

